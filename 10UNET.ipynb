{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a458db",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"UNET: Segmenting Nuclei\"\n",
    "date:   2023-03-09 12:14:54 +0700\n",
    "categories: jekyll update\n",
    "---\n",
    "\n",
    "# TOC\n",
    "\n",
    "- [Definition](#define)\n",
    "- [Code Implementation](#impl)\n",
    "- [Data Augmentation](#aug)\n",
    "- [Filters and Feature maps visualization](#vis)\n",
    "\n",
    "# Definition\n",
    "UNET is a neural network architecture born for the task of segmentation in medical research. Here is the architecture of the UNET from the original paper. They use the usual convolutional layer, max pooling, etc. To the left is the encoder part: to encode image input into number. To the right is the decoder part: to decode and decompress the matrix back into the original size of the image. To not lose the original information along the way, for the decoder branch, they concanate the input with the feature map calculated by the corresponding encoder.\n",
    "\n",
    "<img width=\"714\" alt=\"Screen Shot 2023-03-20 at 10 08 16\" src=\"https://user-images.githubusercontent.com/7457301/226237776-753affb1-9c40-4126-aa0e-8ab2bd50c238.png\">\n",
    "\n",
    "The main principal of UNET is that it labels each pixel whether it belongs to a specific cell/region or not. So it uses the cross entropy loss function. These propensities go through a softmax function as usual to output K regions/classes.\n",
    "\n",
    "$$ L = \\sum_{x \\in \\omega} w(x) log(p_{k(x)} (x)) $$\n",
    "\n",
    "The term w(x) is added to give different weights to different pixels - this is to let the neurons understand bounderies between regions, so that we can have separating lines among nearby cells. Here is the weight map:\n",
    "\n",
    "$$ w(x) = w_{c}(x) + w_0 . exp(-\\frac{(d_1(x) + d_2(x))^2}{2\\sigma^2}) $$\n",
    "\n",
    "with $$ w_c $$ is to balance the class frequencies, and $$ d_1 $$ is the distance to the border of the nearest cell, $$ d_2 $$ is the distance to the border of the second nearest cell. In the original paper, they set $$ w_0=10 $$ and $$ \\sigma \\approx 5 $$ pixels.\n",
    "\n",
    "For initialization, they use a Gaussian distribution with a standard deviation of $$ \\sqrt{\\frac{2}{N}} $$ with N to be the incoming nodees of one neuron. This is to make each feature map to have unit variance.\n",
    "\n",
    "For data that are microscopical images, the images need to endure amidst shifts, rotation, deformation and grayscale variations. They generate smooth deformations using random displacement vectors.\n",
    "\n",
    "# Code implementation\n",
    "\n",
    "Let's examine a kaggle challenge in which we need to color the nuclei in the cell image, using UNET. Examples of the images and its mask:\n",
    "\n",
    "![example3](https://user-images.githubusercontent.com/7457301/226262411-0227d42e-b9a9-4899-8fad-e27a0b93b88e.png)\n",
    "![example2](https://user-images.githubusercontent.com/7457301/226262420-0abd59b8-5198-46bc-a131-c89b1621290a.png)\n",
    "![example1](https://user-images.githubusercontent.com/7457301/226262423-372cb81e-b5e6-4aa9-9bca-9b597bba92bf.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abdddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def conv_block(inputs, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    s = conv_block(inputs, num_filters)\n",
    "    p = MaxPool2D((2, 2))(s)\n",
    "    return s, p\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    \"\"\" Input layer \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    \"\"\" Bottleneck \"\"\"\n",
    "    b1 = conv_block(p4, 1024)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    \"\"\" Output layer \"\"\"\n",
    "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"UNET\")\n",
    "    return model\n",
    "model = build_unet((512, 512, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad1b4a",
   "metadata": {},
   "source": [
    "As metrics, we make use of IOU and Dice coefficient to measure the similarity between prediction and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fff5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Metrics\n",
    "def iou(y_true, y_pred):\n",
    "    def f(y_true, y_pred):\n",
    "        intersection = (y_true * y_pred).sum()\n",
    "        union = y_true.sum() + y_pred.sum() - intersection\n",
    "        x = (intersection + 1e-15) / (union + 1e-15)\n",
    "        x = x.astype(np.float32)\n",
    "        return x\n",
    "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
    "\n",
    "smooth = 1e-15\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "\n",
    "# hyper params\n",
    "batch_size = 4\n",
    "lr = 1e-4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824e9de",
   "metadata": {},
   "source": [
    "For the normal UNET model (without image augmentation), the dice coefficient is 0.83, which shows the high similarity between the true value and the predictive value. We plot some prediction and we can see that there are still issues such as: bordering nuclei are not so good, some nuclei is not color fully (broken inside). We plot the result of UNET together with Canny (a tradditional machine learning algorithm in computer vision, using to trace border)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aeb86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_unet((IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "metrics = [dice_coef, iou, Recall(), Precision()]\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr), metrics=metrics)\n",
    "train_steps = (len(X_train)//batch_size)\n",
    "\n",
    "model.fit(\n",
    "        X_train, Y_train,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=train_steps,\n",
    "    )\n",
    "# dice coef 0.83\n",
    "model.save('model_data_science_bowl_2018.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0caf71",
   "metadata": {},
   "source": [
    "![model_pred1](https://user-images.githubusercontent.com/7457301/226409596-66a07346-1918-4ab8-a9de-2e1e8ff46f4e.png)\n",
    "\n",
    "\n",
    "# Data augmentation\n",
    "\n",
    "When the dataset is small, data augmentation is a common technique to improve accuracy. For example, from one image, we shift it to the right, rotate it a bit, or impose affine transformation. After some simple augmentation of the data, the dice coefficient raise to 0.84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "BATCH_SIZE=4\n",
    "# Creating the training Image and Mask generator\n",
    "image_datagen = image.ImageDataGenerator(shear_range=0.5, rotation_range=50, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='reflect')\n",
    "mask_datagen = image.ImageDataGenerator(shear_range=0.5, rotation_range=50, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='reflect')\n",
    "\n",
    "# Keep the same seed for image and mask generators so they fit together\n",
    "seed=1\n",
    "image_datagen.fit(X_train[:int(X_train.shape[0]*0.9)], augment=True, seed=seed)\n",
    "mask_datagen.fit(Y_train[:int(Y_train.shape[0]*0.9)], augment=True, seed=seed)\n",
    "\n",
    "x=image_datagen.flow(X_train[:int(X_train.shape[0]*0.9)],batch_size=BATCH_SIZE,shuffle=True, seed=seed)\n",
    "y=mask_datagen.flow(Y_train[:int(Y_train.shape[0]*0.9)],batch_size=BATCH_SIZE,shuffle=True, seed=seed)\n",
    "\n",
    "# Checking if the images fit\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "imshow(x.next()[0].astype(np.uint8))\n",
    "plt.show()\n",
    "imshow(np.squeeze(y.next()[0].astype(np.uint8)))\n",
    "plt.show()\n",
    "\n",
    "#creating a training generator that generate masks and images\n",
    "train_generator = zip(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "import tensorflow as tf\n",
    "# earlystopper = EarlyStopping(patience=3, verbose=1)\n",
    "# checkpointer = ModelCheckpoint('model-dsbowl2018-1.h5', verbose=1, save_best_only=True)\n",
    "model2 = build_unet((IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "metrics = [dice_coef, iou, Recall(), Precision()]\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr), metrics=metrics)\n",
    "train_steps = (len(X_train)//batch_size)\n",
    "\n",
    "results = model2.fit_generator(train_generator, steps_per_epoch=train_steps,\n",
    "                              epochs=10)\n",
    "# dice coef 0.84\n",
    "model2.save(\"model_augmeted_data.h5\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2823e5",
   "metadata": {},
   "source": [
    "\n",
    "![model pred](https://user-images.githubusercontent.com/7457301/226409607-d1503329-3c2e-4a58-aded-12713d9c7ce9.png)\n",
    "\n",
    "# Filters and feature maps\n",
    "\n",
    "To understand this neural net more, we plot some of the filters (3x3 matrix mostly) and the feature maps (feature maps are matrix after being multiplied with/convoluted by filters). The followings are the filters of the first convo layer. They look rather simple since in the beginning, the neuron layers are just to realize very simple feature. When we print out the visualization of the first feature map (the input after being transformed/convoluted once), we can see that the initial layers are to look for contrast: they distinguish the in and out of nuclei. When we print out a middle feature map (the input after being convoluted for many times), the images show very stark difference of the borders, only in different direction. So we know that half way through the network, neurons start to be able to border the nuclei, strictly recognising those nuclei. Those images prove that visulization is a great way to see into the black box of neural networks: to see what the neurons see after each layer.\n",
    "\n",
    "![filters](https://user-images.githubusercontent.com/7457301/226409659-c9a8730a-360b-41fc-8459-5ed5b27c5cff.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine model to output right after the first hidden layer\n",
    "ixs = [1,4,8,11,15]\n",
    "outputs = [model.layers[i].output for i in ixs]\n",
    "model = Model(inputs=model.inputs, outputs=outputs)\n",
    "img=X_test[0]\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = np.expand_dims(img, axis=0)\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "# plot the output from each block\n",
    "fmaps = np.squeeze(feature_maps[0])\n",
    "\n",
    "\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "rows = 4\n",
    "columns = 4\n",
    "\n",
    "for i in range (16):\n",
    "  # Adds a subplot at the 1st position\n",
    "  fig.add_subplot(rows, columns, i+1)\n",
    "  # showing image\n",
    "  plt.imshow(fmaps[:,:,i])\n",
    "  plt.axis('off')\n",
    "  \n",
    "\n",
    "plt.savefig('fmap0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5143cb7",
   "metadata": {},
   "source": [
    "\n",
    "![fmap0](https://user-images.githubusercontent.com/7457301/226410410-562c4523-11e2-4cb5-b24a-29739bc36eae.png)\n",
    "\n",
    "![fmap2](https://user-images.githubusercontent.com/7457301/226410393-16db35e3-2caa-4799-a4ee-45b845f8788f.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc900a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
